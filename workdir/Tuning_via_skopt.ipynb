{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "\n",
    "from pythiamill import PythiaMill\n",
    "from pythiamill.utils import TuneMCDetector\n",
    "import time\n",
    "\n",
    "from skopt import Optimizer\n",
    "from skopt.space import Real\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading parameters' description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('params/all_pars_dict.json', 'r') as iofile:\n",
    "    all_param_info = json.load(iofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here active blocks are selected. No more bool values, just provide target block numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_blocks = [3]\n",
    "\n",
    "active_param_list = []\n",
    "for block_number in active_blocks:\n",
    "    for par in all_param_info:\n",
    "        if int(all_param_info[par]['block'])==block_number:\n",
    "            active_param_list.append(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get target parameters constraints. Now only `float` values are used, so other types will raise `ValueError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_param_info_dict = dict()\n",
    "for par in active_param_list:\n",
    "    par_info = {\n",
    "        'type': 'FLOAT',\n",
    "        'size': 1,\n",
    "        'min': all_param_info[par]['min'],\n",
    "        'max':all_param_info[par]['max']\n",
    "    }\n",
    "    \n",
    "    active_param_info_dict[par]=par_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decupletSup': {'max': 1.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'etaPrimeSup': {'max': 1.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'etaSup': {'max': 1.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'mesonBvector': {'max': 3.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'mesonCvector': {'max': 3.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'mesonSvector': {'max': 3.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'mesonUDvector': {'max': 3.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'probQQ1toQQ0': {'max': 1.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'probQQtoQ': {'max': 1.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'probSQtoQQ': {'max': 1.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'},\n",
       " 'probStoUD': {'max': 1.0, 'min': 0.001, 'size': 1, 'type': 'FLOAT'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_param_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimization process `skopt` needs dimensions in special format. Next function generates it from the info dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_space(variables_dict):\n",
    "    space = []\n",
    "    for var_name, params in variables_dict.items():\n",
    "        if params['type'] == 'FLOAT':\n",
    "            space.append(Real(name=var_name, low=params[u'min'], high=params[u'max']))\n",
    "        else:\n",
    "            raise ValueError\n",
    "    return space\n",
    "\n",
    "dimensions = extract_features_space(active_param_info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythia needs some options to run the experiment. These options are the same as were used in TuneMC experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('params/pythia_params_prefix.json', 'r') as iofile:\n",
    "    pythia_params_prefix = json.load(iofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the reference solution based on Monash 13 experiment data (https://arxiv.org/pdf/1404.5630.pdf page 41). These values are already in `all_param_info` dict with key `Monash`. \n",
    "\n",
    "Options for sampling are combined from prefix mentioned above and selected parameters (block 1, 2 or 3) values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_solution = {\n",
    "    x: '{}:{} = {}'.format(y['category'], x, y['Monash']) for x, y in all_param_info.items()\n",
    "}\n",
    "\n",
    "reference_options = []\n",
    "reference_options.extend(pythia_params_prefix)\n",
    "for par in active_param_list:\n",
    "    reference_options.append(reference_solution[par])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tune:ee = 7',\n",
       " 'Beams:idA = 11',\n",
       " 'Beams:idB = -11',\n",
       " 'Beams:eCM = 91.2',\n",
       " 'WeakSingleBoson:ffbar2gmZ = on',\n",
       " '23:onMode = off',\n",
       " '23:onIfMatch = 1 -1',\n",
       " '23:onIfMatch = 2 -2',\n",
       " '23:onIfMatch = 3 -3',\n",
       " '23:onIfMatch = 4 -4',\n",
       " '23:onIfMatch = 5 -5',\n",
       " 'StringFlav:decupletSup = 1.0',\n",
       " 'StringFlav:etaPrimeSup = 1.0',\n",
       " 'StringFlav:etaSup = 0.6',\n",
       " 'StringFlav:mesonBvector = 2.2',\n",
       " 'StringFlav:mesonCvector = 0.88',\n",
       " 'StringFlav:mesonSvector = 0.55',\n",
       " 'StringFlav:mesonUDvector = 0.5',\n",
       " 'StringFlav:probQQ1toQQ0 = 0.0275',\n",
       " 'StringFlav:probQQtoQ = 0.081',\n",
       " 'StringFlav:probSQtoQQ = 0.915',\n",
       " 'StringFlav:probStoUD = 0.217']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = TuneMCDetector()\n",
    "BATCH_SIZE = 1\n",
    "N_WORKERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_for_mill(tuned_params):\n",
    "    global pythia_params_prefix\n",
    "    global dimensions\n",
    "\n",
    "    params = []\n",
    "    params.extend(pythia_params_prefix)\n",
    "    for idx, dim in enumerate(dimensions):\n",
    "        param_string = \"{}:{} = {}\".format(all_param_info[dim.name]['category'], dim.name, tuned_params[idx])\n",
    "        params.append(param_string)\n",
    "    return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_with_params(options, n_batches=1):\n",
    "    global detector, BATCH_SIZE, N_WORKERS\n",
    "    mill = PythiaMill(detector, options, cache_size=16, batch_size=BATCH_SIZE, n_workers=N_WORKERS, seed=123)\n",
    "\n",
    "#     data = np.vstack([\n",
    "#         mill.sample() for _ in tqdm_notebook(range(n_batches), leave=False)\n",
    "#     ])\n",
    "    data = np.vstack([\n",
    "        mill.sample() for _ in range(n_batches)\n",
    "    ])\n",
    "\n",
    "    mill.terminate()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axis 0 is sample number within batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_solution = np.mean(sample_points_with_params(reference_options, 100), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of $\\chi^2$ using JS-divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target function should be in certain format to work with `skopt` class `Optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_mill_quality_func(tuned_params):\n",
    "    global reference_solution\n",
    "    global tqdm_object\n",
    "\n",
    "    options = generate_input_for_mill(tuned_params)\n",
    "    \n",
    "    s_time = time.time()\n",
    "    generated = np.mean(sample_points_with_params(options, 10), axis=0)\n",
    "    jsd = JSD(generated, reference_solution)\n",
    "    e_time = time.time()\n",
    "    tqdm_object.update(1)\n",
    "    tqdm_object.set_postfix(current_jsd=jsd, elapsed_time='{:.2f} s'.format(e_time - s_time))\n",
    "    return jsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INITIAL_POINTS = 15\n",
    "N_STEPS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf09046f18c461ab2de99e5ec8386ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "tqdm_object = tqdm_notebook(total=N_STEPS)\n",
    "tqdm_object.set_description(desc='Total:')\n",
    "opt = Optimizer(dimensions=dimensions, n_initial_points=N_INITIAL_POINTS)\n",
    "res = opt.run(wrapped_mill_quality_func, N_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
